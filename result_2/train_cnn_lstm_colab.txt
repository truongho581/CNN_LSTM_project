# -*- coding: utf-8 -*-
"""train_cnn_lstm_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17yMhDiVGELUeq4nCnBe8ENjnN-3ZZck0
"""

import shutil
import os

data_dirs = ['data/train', 'data/val', 'data/test']

for dir_path in data_dirs:
    if os.path.exists(dir_path):
        shutil.rmtree(dir_path)  # X√≥a c·∫£ th∆∞ m·ª•c con v√† file b√™n trong
        os.makedirs(dir_path)    # T·∫°o l·∫°i th∆∞ m·ª•c r·ªóng

from google.colab import drive
drive.mount('/content/drive')

# üì¶ Gi·∫£i n√©n d·ªØ li·ªáu
!unzip -q /content/drive/MyDrive/data_1.zip

# ‚úÖ C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt
!pip install tensorflow matplotlib numpy

# ‚úÖ Load d·ªØ li·ªáu t·ª´ th∆∞ m·ª•c
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

img_size = (128, 128)
batch_size = 64

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# ‚úÖ Data Augmentation chu·∫©n cho spectrogram ƒë·ªông ƒë·∫•t
train_gen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,        # Xoay nh·∫π ¬±20 ƒë·ªô
    width_shift_range=0.2,    # D·ªãch ngang ¬±20%
    height_shift_range=0.2,   # D·ªãch d·ªçc ¬±20%
    zoom_range=0.2,           # Zoom ¬±20%
    shear_range=0.1,          # Bi·∫øn d·∫°ng ch√©o nh·∫π
    horizontal_flip=False,    # KH√îNG flip ngang (gi·ªØ tr·ª•c th·ªùi gian ƒë√∫ng)
    fill_mode='nearest'
)

# ‚úÖ Generator cho validation ch·ªâ rescale, kh√¥ng augment
val_gen = ImageDataGenerator(rescale=1./255)

# ‚úÖ Load data
train_data = train_gen.flow_from_directory(
    'data/train',
    target_size=img_size,
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='binary',
    shuffle=True
)

val_data = val_gen.flow_from_directory(
    'data/val',
    target_size=img_size,
    color_mode='grayscale',
    batch_size=batch_size,
    class_mode='binary',
    shuffle=False
)

from tensorflow.keras.models import Model
from tensorflow.keras.layers import (
    Input, Conv2D, MaxPooling2D, BatchNormalization,
    Activation, Reshape, LSTM, Bidirectional,
    Dense, Dropout
)

inputs = Input(shape=(128, 128, 1))

# Block 1
x = Conv2D(32, (3, 3), padding='same')(inputs)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2, 2))(x)  # (64, 64, 32)
x = Dropout(0.25)(x)

# Block 2
x = Conv2D(64, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2, 2))(x)  # (32, 32, 64)
x = Dropout(0.25)(x)

# Block 3 (b·ªï sung chi·ªÅu s√¢u)
x = Conv2D(128, (3, 3), padding='same')(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = MaxPooling2D((2, 2))(x)  # (16, 16, 128)
x = Dropout(0.25)(x)

# Reshape cho LSTM
x = Reshape((16, 16 * 128))(x)  # (16, 2048)

# Bidirectional LSTM (ho·∫∑c stack 2 LSTM)
x = Bidirectional(LSTM(64, return_sequences=True))(x)
x = Bidirectional(LSTM(32))(x)

x = Dropout(0.5)(x)

# Fully Connected
outputs = Dense(1, activation='sigmoid')(x)

model = Model(inputs, outputs)

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger

checkpoint = ModelCheckpoint(
    filepath='best_model_epoch{epoch:02d}_valAcc{val_accuracy:.4f}.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

earlystop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    verbose=1,
    min_lr=1e-6
)

csv_logger = CSVLogger('training_log.csv', append=True)

history = model.fit(
    train_data,
    epochs=50,
    validation_data=val_data,
    callbacks=[checkpoint, earlystop, reduce_lr, csv_logger]
)

import os
import re
from tensorflow.keras.models import load_model

# ‚úÖ 1) Li·ªát k√™ t·∫•t c·∫£ file .h5
model_files = [f for f in os.listdir() if f.endswith('.h5')]

# ‚úÖ 2) Extract val_acc t·ª´ t√™n file
# Modified pattern to correctly capture the float value without the trailing dot
pattern = re.compile(r'valAcc([0-9.]+)\.h5')

models_with_acc = []
for f in model_files:
    match = pattern.search(f)
    if match:
        val_acc = float(match.group(1))
        models_with_acc.append( (f, val_acc) )

# ‚úÖ 3) T√¨m file c√≥ val_acc cao nh·∫•t
if models_with_acc:
    best_model_file = max(models_with_acc, key=lambda x: x[1])[0]
    print(f"‚úÖ Best model found: {best_model_file}")

    # ‚úÖ 4) Load model
    best_model = load_model(best_model_file)
    print("‚úÖ Model loaded successfully.")

    # Evaluate
    # Check if test_data is defined, if not, create it
    if 'test_data' not in globals():
        print("‚ÑπÔ∏è Creating test_data generator...")
        test_gen = ImageDataGenerator(rescale=1./255)
        test_data = test_gen.flow_from_directory(
            'data/test',
            target_size=img_size,
            color_mode='grayscale',
            batch_size=batch_size,
            class_mode='binary',
            shuffle=False
        )


    loss, acc = best_model.evaluate(test_data)
    print(f"üéØ Test Accuracy: {acc*100:.2f}%")

    # Predict
    y_pred_probs = best_model.predict(test_data).flatten()
    y_pred = (y_pred_probs > 0.5).astype(int)

    # True labels
    # Need to reset the test_data generator to get labels in correct order for evaluation
    test_data.reset()
    y_true = test_data.classes

    # Classification report
    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score

    print("\nüìä Classification Report:")
    print(classification_report(y_true, y_pred, target_names=['Noise', 'Earthquake']))

    # Confusion matrix heatmap
    import seaborn as sns
    import matplotlib.pyplot as plt

    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(5, 4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Noise', 'Earthquake'], yticklabels=['Noise', 'Earthquake'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # ROC AUC
    auc = roc_auc_score(y_true, y_pred_probs)
    print(f"\nüî¨ ROC AUC: {auc:.4f}")

    # Probability histogram
    plt.figure()
    plt.hist(y_pred_probs, bins=50)
    plt.title('Prediction Probability Distribution')
    plt.xlabel('Predicted Probability')
    plt.ylabel('Frequency')
    plt.show()

else:
    print("‚ùå No valid model file found.")

import matplotlib.pyplot as plt

# plt.style.use('seaborn-whitegrid') # Removed this line
plt.figure(figsize=(12, 5))

# üéØ Accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Val Accuracy', marker='s')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim(0, 1)
plt.legend()
plt.grid(True)

# üìâ Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Val Loss', marker='s')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('learning_curve.png', dpi=300)
plt.show()

# 1. Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# 2. T√¨m file checkpoint m·ªõi nh·∫•t
import os, glob, shutil
list_of_files = glob.glob('*.h5')
latest_file = max(list_of_files, key=os.path.getctime)
print(f"‚úÖ Latest: {latest_file}")

# 3. Copy sang Drive
save_dir = '/content/drive/MyDrive/Colab_models/'
os.makedirs(save_dir, exist_ok=True)
shutil.copy(latest_file, save_dir)
print(f"üöÄ Saved {latest_file} to {save_dir}")